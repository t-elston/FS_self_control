{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# single_neuron_processing_one_hot\n",
    "Assesses neuron by neuron data for each recording and then saves into summary files\n",
    "- uses a one-hot encoding regression to identify selective neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports \n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import matplotlib.colors as mcolors\n",
    "import warnings\n",
    "import pingouin as pg\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import statsmodels.formula.api as smf\n",
    "import glob\n",
    "import os\n",
    "warnings.filterwarnings(\"ignore\", message=\"Mean of empty slice\", category=RuntimeWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions\n",
    "def get_labelled_posteriors(indata, labels):\n",
    "\n",
    "    '''\n",
    "    INPUTS:\n",
    "    indata = posterior probabilites from a classifier with the shape\n",
    "            n_trials x n_timesteps x n_classes\n",
    "        \n",
    "    labels = 1d array with len(n_trials) - these labels ought\n",
    "            to correspond to class numbers (layers in indata)\n",
    "\n",
    "    OUTPUT:\n",
    "        labelled_posteriors = posterior probabilities associated with the\n",
    "        classes in the labels input for each timestep and trial\n",
    "    '''\n",
    "\n",
    "    n_trials, n_times, n_classes = indata.shape\n",
    "    class_lbls = np.unique(labels)\n",
    "    class_lbls = class_lbls[~np.isnan(class_lbls)]\n",
    "\n",
    "    # initialize output\n",
    "    labelled_posteriors = np.zeros(shape = (n_trials, n_times))\n",
    "\n",
    "    for ix, lbl in enumerate(class_lbls):\n",
    "        \n",
    "        # find trials where this label was chosen\n",
    "        labelled_posteriors[labels == lbl,:] = indata[labels == lbl,:,int(ix)]\n",
    "        \n",
    "    return labelled_posteriors\n",
    "\n",
    "\n",
    "def pull_balanced_train_set(trials2balance, params2balance):\n",
    "    '''\n",
    "    INPUTS:\n",
    "    trials2balance   - ***logical array*** of the trials you want to balance\n",
    "    params2balance   - ***list*** where each element is a vector of categorical\n",
    "                        parameters to balance (e.g. choice value and side)\n",
    "                        each element of params2balance must have the same\n",
    "                        number of elements as trials2balance\n",
    "    OUTPUTS:\n",
    "    train_ix         - trial indices of a fully balanced training set\n",
    "    leftover_ix      - trial indices of trials not included in train_ix\n",
    "    '''\n",
    "\n",
    "    # Find the indices where trials are selected to balance\n",
    "    balance_indices = np.where(trials2balance)[0]\n",
    "\n",
    "    # Create an array of parameters to balance\n",
    "    params_array = np.array(params2balance).T\n",
    "\n",
    "    # Find unique combinations and their counts\n",
    "    p_combos, p_counts = np.unique(params_array[balance_indices], axis=0, return_counts=True)\n",
    "\n",
    "    # Determine the minimum count for a balanced set\n",
    "    n_to_keep = np.min(p_counts)\n",
    "\n",
    "    # Initialize arrays to mark selected and leftover trials\n",
    "    train_ix = np.zeros(len(trials2balance), dtype=bool)\n",
    "    leftover_ix = np.zeros(len(trials2balance), dtype=bool)\n",
    "\n",
    "    # Select a balanced number of trials for each unique parameter combination\n",
    "    for combo in p_combos:\n",
    "        # Find indices of trials corresponding to the current combination\n",
    "        combo_indices = np.where((params_array == combo).all(axis=1) & trials2balance)[0]\n",
    "\n",
    "        # Shuffle the indices\n",
    "        np.random.shuffle(combo_indices)\n",
    "\n",
    "        # Select n_to_keep trials and mark them as part of the training set\n",
    "        train_ix[combo_indices[:n_to_keep]] = True\n",
    "\n",
    "        # Mark the remaining trials as leftovers\n",
    "        leftover_ix[combo_indices[n_to_keep:]] = True\n",
    "\n",
    "    return train_ix, leftover_ix\n",
    "\n",
    "\n",
    "def random_prop_of_array(inarray, proportion):\n",
    "    '''\n",
    "    INPUTS\n",
    "    inarray = logical/boolean array of indices to potentially use later\n",
    "    proportion = how much of inarray should randomly be selected\n",
    "\n",
    "    OUTPUT\n",
    "    out_array = logical/boolean that's set as 'true' for a proportion of the \n",
    "                initial 'true' values in inarray\n",
    "    '''\n",
    "\n",
    "    out_array = np.zeros(shape = (len(inarray), ))\n",
    "\n",
    "    # find where inarray is true and shuffle those indices\n",
    "    shuffled_ixs = np.random.permutation(np.asarray(np.where(inarray)).flatten())\n",
    "\n",
    "    # keep only a proportion of that array\n",
    "    kept_ix = shuffled_ixs[0: round(len(shuffled_ixs)*proportion)]\n",
    "\n",
    "    # fill in the kept indices\n",
    "    out_array[kept_ix] = 1\n",
    "\n",
    "    # make this a logical/boolean\n",
    "    out_array = out_array > 0\n",
    "\n",
    "    return out_array\n",
    "\n",
    "\n",
    "def pull_from_h5(file_path, data_to_extract):\n",
    "    try:\n",
    "        with h5py.File(file_path, 'r') as file:\n",
    "            # Check if the data_to_extract exists in the HDF5 file\n",
    "            if data_to_extract in file:\n",
    "                data = file[data_to_extract][...]  # Extract the data\n",
    "                return data\n",
    "            else:\n",
    "                print(f\"'{data_to_extract}' not found in the file.\")\n",
    "                return None\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None\n",
    "    \n",
    "def list_hdf5_data(file_path):\n",
    "    try:\n",
    "        with h5py.File(file_path, 'r') as file:\n",
    "            print(f\"Datasets in '{file_path}':\")\n",
    "            for dataset in file:\n",
    "                print(dataset)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "\n",
    "def get_ch_and_unch_vals(bhv):\n",
    "    \"\"\"\n",
    "    Extracts chosen (ch_val) and unchosen (unch_val) values associated with each trial.\n",
    "\n",
    "    Parameters:\n",
    "    - bhv (DataFrame): DataFrame behavioral data.\n",
    "\n",
    "    Returns:\n",
    "    - ch_val (ndarray): Array of chosen values for each trial.\n",
    "    - unch_val (ndarray): Array of unchosen values for each trial. \n",
    "                          - places 0s for unchosen values on forced choice trials\n",
    "    \"\"\"\n",
    "    ch_val = np.zeros(shape=(len(bhv, )))\n",
    "    unch_val = np.zeros(shape=(len(bhv, )))\n",
    "\n",
    "    bhv['r_val'] = bhv['r_val'].fillna(0)\n",
    "    bhv['l_val'] = bhv['l_val'].fillna(0)\n",
    "\n",
    "    ch_left = bhv['side'] == -1\n",
    "    ch_right = bhv['side'] == 1\n",
    "\n",
    "    ch_val[ch_left] = bhv['l_val'].loc[ch_left].astype(int)\n",
    "    ch_val[ch_right] = bhv['r_val'].loc[ch_right].astype(int)\n",
    "\n",
    "    unch_val[ch_left] = bhv['r_val'].loc[ch_left].astype(int)\n",
    "    unch_val[ch_right] = bhv['l_val'].loc[ch_right].astype(int)\n",
    "\n",
    "    return ch_val, unch_val\n",
    "\n",
    "\n",
    "def get_ch_and_unch_pps(in_pp, bhv, ch_val, unch_val):\n",
    "    \"\"\"Gets the posteriors associated with the chosen and unchosen classes\n",
    "\n",
    "    Args:\n",
    "        in_pp (ndarray): array of posteriors (n_trials x n_times x n_classes)\n",
    "        bhv (dataframe): details of each trial\n",
    "        ch_val (ndarray): vector indicating the class that is ultimately chosen\n",
    "        unch_val (ndarray): vector indicating the class that was ultimately not chosen\n",
    "\n",
    "    Returns:\n",
    "        ch_pp (ndarray): vector of the postior at each point in time for each trial's chosen option\n",
    "        unch_pp (ndarray): vector of the postior at each point in time for each trial's unchosen option\n",
    "    \"\"\"\n",
    "\n",
    "    # select the chosen and unchosen values \n",
    "    n_trials, n_times, n_classes = np.shape(in_pp)\n",
    "    ch_pp = np.zeros(shape=(n_trials, n_times))\n",
    "    unch_pp = np.zeros(shape=(n_trials, n_times))\n",
    "\n",
    "    # loop over each trial\n",
    "    for t in range(n_trials):\n",
    "        \n",
    "        # get the chosen and unchosen PPs\n",
    "        ch_pp[t, :] = in_pp[t, :, int(ch_val[t]-1)]\n",
    "        unch_pp[t, :] = in_pp[t, :, int(unch_val[t]-1)]\n",
    "        \n",
    "    # set the forced choice unchosen pps to nans, since there was only 1 option\n",
    "    unch_pp[bhv['forced'] == 1, :] = np.nan\n",
    "    \n",
    "    return ch_pp, unch_pp\n",
    "\n",
    "\n",
    "def get_alt_ch_and_unch_pps(in_pp, bhv, s_ch_val, s_unch_val):\n",
    "    \"\"\"Gets the posteriors associated with the chosen and unchosen classes\n",
    "\n",
    "    Args:\n",
    "        in_pp (ndarray): array of posteriors (n_trials x n_times x n_classes)\n",
    "        bhv (dataframe): details of each trial\n",
    "        s_ch_val (ndarray): vector indicating the class that is ultimately chosen\n",
    "        s_unch_val (ndarray): vector indicating the class that was ultimately not chosen\n",
    "\n",
    "    Returns:\n",
    "        alt_ch_pp (ndarray): vector of the postior at each point in time for the alternative value in the other state\n",
    "        alt_unch_pp (ndarray): vector of the postior at each point in time for the alternative value in the other state\n",
    "    \"\"\"\n",
    "\n",
    "    # select the chosen and unchosen values \n",
    "    n_trials, n_times, n_classes = np.shape(in_pp)\n",
    "    alt_ch_pp = np.zeros(shape=(n_trials, n_times))\n",
    "    alt_unch_pp = np.zeros(shape=(n_trials, n_times))\n",
    "\n",
    "    alt_ch_val = np.zeros_like(s_ch_val)\n",
    "    alt_unch_val = np.zeros_like(s_unch_val)\n",
    "    \n",
    "    alt_ch_val[bhv['state'] == 1] = 8 - s_ch_val[bhv['state'] == 1] + 1\n",
    "    alt_ch_val[bhv['state'] == 2] = 8 - s_ch_val[bhv['state'] == 2] + 1\n",
    "\n",
    "    alt_unch_val[bhv['state'] == 1] = 8 - s_unch_val[bhv['state'] == 1] + 1\n",
    "    alt_unch_val[bhv['state'] == 2] = 8 - s_unch_val[bhv['state'] == 2] + 1\n",
    "\n",
    "    for t in range(n_trials):\n",
    "        \n",
    "        alt_ch_pp[t, :] = in_pp[t, :, int(alt_ch_val[t]-1)]\n",
    "        alt_unch_pp[t, :] = in_pp[t, :, int(alt_unch_val[t]-1)]\n",
    "\n",
    "    # set the alternative values to nans for state 3, since there were no alternatives\n",
    "    alt_ch_pp[bhv['state'] == 3] = np.nan\n",
    "    alt_unch_pp[bhv['state'] == 3] = np.nan\n",
    "\n",
    "    return alt_ch_pp, alt_unch_pp\n",
    "\n",
    "def find_candidate_states(indata, n_classes, temporal_thresh, mag_thresh):\n",
    "    \"\"\"Finds periods where decoded posteriors are twice their noise level.\n",
    "\n",
    "    Args:\n",
    "        indata (ndarray): 2d array of posterior probabilities associated with some decoder output.\n",
    "        n_classes (int): How many classes were used in the decoder?\n",
    "        temporal_thresh (int): Number of contiguous samples that must be above a threshold to be a real state (typically 2).\n",
    "        mag_thresh (flat): how many times the noise level must a state be? (e.g. 2 = twice the noise level)\n",
    "\n",
    "    Returns:\n",
    "        state_details (ndarray): 2d array where each row details when a state occurred [trial_num, time_in_trial, state_length].\n",
    "        state_array (ndarray): 2d array the same size as indata. It contains 1 in all locations where there were states and 0s everywhere else.\n",
    "    \"\"\"\n",
    "    state_details = np.array([])\n",
    "    state_array = np.zeros_like(indata)\n",
    "    \n",
    "    state_magnitude_thresh = (1 / n_classes) * mag_thresh\n",
    "\n",
    "    for t in range(indata.shape[0]):\n",
    "        state_len, state_pos, state_type = find_1dsequences(indata[t, :] > state_magnitude_thresh)\n",
    "        state_len = state_len[state_type == True]\n",
    "        state_pos = state_pos[state_type == True]\n",
    "\n",
    "        for i in range(len(state_len)):\n",
    "            state_details = np.concatenate((state_details, np.array([t, state_pos[i], state_len[i]])))\n",
    "\n",
    "    state_details = state_details.reshape(-1, 3)\n",
    "    state_details = state_details[state_details[:, 2] > temporal_thresh, :]\n",
    "\n",
    "    # Update state_array using state_details information\n",
    "    for j in range(len(state_details)):\n",
    "        state_trial, state_start, state_len = state_details[j].astype(int)\n",
    "        state_array[state_trial, state_start:(state_start + state_len)] = 1\n",
    "\n",
    "    return state_details, state_array\n",
    "\n",
    "def moving_average(x, w, axis=0):\n",
    "    '''\n",
    "    Moving average function that operates along specified dimensions of a NumPy array.\n",
    "\n",
    "    Parameters:\n",
    "    - x (numpy.ndarray): Input array.\n",
    "    - w (int): Size of the window to convolve the array with (i.e., smoothness factor).\n",
    "    - axis (int): Axis along which to perform the moving average (default is 0).\n",
    "\n",
    "    Returns:\n",
    "    - numpy.ndarray: Smoothed array along the specified axis with the same size as the input array.\n",
    "    '''\n",
    "    x = np.asarray(x)  # Ensure input is a NumPy array\n",
    "    if np.isnan(x).any():\n",
    "        x = np.nan_to_num(x)  # Replace NaN values with zeros\n",
    "\n",
    "    if axis < 0:\n",
    "        axis += x.ndim  # Adjust negative axis value\n",
    "\n",
    "    kernel = np.ones(w) / w  # Create kernel for moving average\n",
    "\n",
    "    # Pad the array before applying convolution\n",
    "    pad_width = [(0, 0)] * x.ndim  # Initialize padding for each axis\n",
    "    pad_width[axis] = (w - 1, 0)  # Pad along the specified axis (left side)\n",
    "    x_padded = np.pad(x, pad_width, mode='constant', constant_values=0)\n",
    "\n",
    "    # Apply 1D convolution along the specified axis on the padded array\n",
    "    return np.apply_along_axis(lambda m: np.convolve(m, kernel, mode='valid'), axis, x_padded)\n",
    "\n",
    "def find_1dsequences(inarray):\n",
    "        ''' \n",
    "        run length encoding. Partial credit to R rle function. \n",
    "        Multi datatype arrays catered for including non Numpy\n",
    "        returns: tuple (runlengths, startpositions, values) \n",
    "        '''\n",
    "        ia = np.asarray(inarray)                # force numpy\n",
    "        n = len(ia)\n",
    "        if n == 0: \n",
    "            return (None, None, None)\n",
    "        else:\n",
    "            y = ia[1:] != ia[:-1]                 # pairwise unequal (string safe)\n",
    "            i = np.append(np.where(y), n - 1)     # must include last element \n",
    "            lens = np.diff(np.append(-1, i))      # run lengths\n",
    "            pos = np.cumsum(np.append(0, lens))[:-1] # positions\n",
    "            return(lens, pos, ia[i])\n",
    "        \n",
    "        \n",
    "def calculate_mean_and_interval(data, type='sem', num_samples=1000, alpha=0.05):\n",
    "    \"\"\"\n",
    "    Calculate mean and either SEM or bootstrapped CI for each column of the input array, disregarding NaN values.\n",
    "\n",
    "    Parameters:\n",
    "    - data: 2D numpy array\n",
    "    - type: str, either 'sem' or 'bootstrap_ci'\n",
    "    - num_samples: int, number of bootstrap samples (applicable only for type='bootstrap_ci')\n",
    "    - alpha: float, significance level for the confidence interval (applicable only for type='bootstrap_ci')\n",
    "\n",
    "    Returns:\n",
    "    - means: 1D numpy array containing means for each column\n",
    "    - interval: 1D numpy array containing SEMs or bootstrapped CIs for each column\n",
    "    \"\"\"\n",
    "    nan_mask = ~np.isnan(data)\n",
    "    \n",
    "    nanmean_result = np.nanmean(data, axis=0)\n",
    "    n_valid_values = np.sum(nan_mask, axis=0)\n",
    "    \n",
    "    if type == 'sem':\n",
    "        nanstd_result = np.nanstd(data, axis=0)\n",
    "        interval = nanstd_result / np.sqrt(n_valid_values)\n",
    "        \n",
    "    elif type == 'percentile':\n",
    "        interval = np.mean(np.array([np.abs(nanmean_result - np.nanpercentile (data, 5, axis=0)), np.abs(nanmean_result - np.nanpercentile (data, 95, axis=0))]))\n",
    "        \n",
    "        \n",
    "    elif type == 'bootstrap':\n",
    "        n_rows, n_cols = data.shape\n",
    "\n",
    "        # Initialize array to store bootstrap means\n",
    "        bootstrap_means = np.zeros((num_samples, n_cols))\n",
    "\n",
    "        # Perform bootstrap resampling for each column\n",
    "        for col in range(n_cols):\n",
    "            bootstrap_samples = np.random.choice(data[:, col][nan_mask[:, col]], size=(num_samples, n_rows), replace=True)\n",
    "            bootstrap_means[:, col] = np.mean(bootstrap_samples, axis=1)\n",
    "\n",
    "        # Calculate confidence interval bounds\n",
    "        ci_lower = np.percentile(bootstrap_means, 100 * (alpha / 2), axis=0)\n",
    "        ci_upper = np.percentile(bootstrap_means, 100 * (1 - alpha / 2), axis=0)\n",
    "        \n",
    "        interval = np.mean([abs(bootstrap_means - ci_lower), abs(bootstrap_means - ci_upper)], axis=0)\n",
    "        \n",
    "        interval = np.mean(interval, axis=0)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Invalid 'type' argument. Use either 'sem' or 'bootstrap'.\")\n",
    "    \n",
    "    return nanmean_result, interval\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all the files in the directory\n",
    "datadir = 'C:/Users/thome/Documents/PYTHON/OFC-CdN 3 state self control/files_for_decoder/'\n",
    "data_files = glob.glob(os.path.join(datadir, '*.h5'))\n",
    "file_names = [os.path.basename(file) for file in data_files]\n",
    "\n",
    "save_dir = 'C:/Users/thome/Documents/PYTHON/OFC-CdN 3 state self control/single_neuron_summary/' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D20231219_Rec05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 787/787 [06:24<00:00,  2.05it/s]\n",
      "100%|██████████| 787/787 [05:06<00:00,  2.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving data...\n",
      "Data saved \n",
      "\n",
      "D20231221_Rec06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 606/606 [05:08<00:00,  1.96it/s]\n",
      "100%|██████████| 606/606 [04:02<00:00,  2.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving data...\n",
      "Data saved \n",
      "\n",
      "D20231224_Rec07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 503/503 [04:14<00:00,  1.98it/s]\n",
      "100%|██████████| 503/503 [03:22<00:00,  2.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving data...\n",
      "Data saved \n",
      "\n",
      "D20231227_Rec08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 546/546 [04:35<00:00,  1.98it/s]\n",
      "100%|██████████| 546/546 [03:26<00:00,  2.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving data...\n",
      "Data saved \n",
      "\n",
      "K20240707_Rec06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 557/557 [04:45<00:00,  1.95it/s]\n",
      "100%|██████████| 557/557 [03:43<00:00,  2.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving data...\n",
      "Data saved \n",
      "\n",
      "K20240710_Rec07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 930/930 [09:10<00:00,  1.69it/s]\n",
      "100%|██████████| 930/930 [06:16<00:00,  2.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving data...\n",
      "Data saved \n",
      "\n",
      "K20240712_Rec08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 520/520 [04:24<00:00,  1.96it/s]\n",
      "100%|██████████| 520/520 [03:27<00:00,  2.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving data...\n",
      "Data saved \n",
      "\n",
      "K20240715_Rec09\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 606/606 [05:01<00:00,  2.01it/s]\n",
      "100%|██████████| 606/606 [03:51<00:00,  2.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving data...\n",
      "Data saved \n",
      "\n",
      "All files processed :]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# loop over each file\n",
    "for f_ix, file_path in enumerate(data_files):\n",
    "    \n",
    "    print(file_names[f_ix][0:-3]) \n",
    "\n",
    "    # access the data for this session\n",
    "    firing_rates = np.concatenate([pull_from_h5(file_path, 'CdN_zFR'), \n",
    "                                pull_from_h5(file_path, 'OFC_zFR')], axis=2)\n",
    "\n",
    "    u_names = np.concatenate([pull_from_h5(file_path, 'CdN_u_names'), \n",
    "                            pull_from_h5(file_path, 'OFC_u_names')], axis=0)\n",
    "\n",
    "    n_OFC = pull_from_h5(file_path, 'OFC_zFR').shape[2]\n",
    "    n_CdN = pull_from_h5(file_path, 'CdN_zFR').shape[2]\n",
    "    brain_areas = np.concatenate([np.zeros(shape=n_CdN, ), np.ones(shape=n_OFC, )]).astype(int)\n",
    "\n",
    "    ts = pull_from_h5(file_path, 'ts')\n",
    "    bhv = pd.read_hdf(file_path, key='bhv')\n",
    "\n",
    "    if len(bhv) > len(firing_rates):\n",
    "        bhv = bhv.loc[0 :len(firing_rates)-1]\n",
    "\n",
    "    # subselect trials with a response that was correct\n",
    "    trials2keep = (bhv['n_sacc'] > 0)\n",
    "    bhv = bhv.loc[trials2keep]\n",
    "    firing_rates = firing_rates[trials2keep, :,:]\n",
    "    firing_rates = np.nan_to_num(firing_rates, nan=0)\n",
    "\n",
    "    n_trials, n_times, n_units = np.shape(firing_rates)\n",
    "    \n",
    "    # get firing rates during choice epoch\n",
    "    choice_on = np.argwhere(ts == 0)[0][0]\n",
    "    choice_off = np.argwhere(ts ==300)[0][0]\n",
    "\n",
    "    cue_on = np.argwhere(ts == -500)[0][0]\n",
    "    cue_off = np.argwhere(ts == -100)[0][0]\n",
    "\n",
    "    choice_frs = np.mean(firing_rates[:,choice_on:choice_off,:], axis=1)\n",
    "    cue_frs = np.mean(firing_rates[:,cue_on:cue_off,:], axis=1)\n",
    "\n",
    "    ix = (bhv['n_sacc'] ==1)\n",
    "    n_units = np.size(choice_frs, 1)\n",
    "\n",
    "    # create indices for the states of each trial\n",
    "    s1_ix = bhv['state'] == 1\n",
    "    s2_ix = bhv['state'] == 2\n",
    "    s3_ix = bhv['state'] == 3\n",
    "\n",
    "    n_times = len(ts)\n",
    "\n",
    "    # create factor array for regression analysis where the factors are encoded in a one-hot manner\n",
    "    one_hot_reg_factors = pd.DataFrame(index=bhv.index)\n",
    "    one_hot_reg_factors['state_a'] = np.zeros_like(bhv['state'])\n",
    "    one_hot_reg_factors.loc[bhv['state'] == 1, 'state_a'] = 1\n",
    "    one_hot_reg_factors['state_b'] = np.zeros_like(bhv['state'])\n",
    "    one_hot_reg_factors.loc[bhv['state'] == 2, 'state_b'] = 1\n",
    "    one_hot_reg_factors['state_c'] = np.zeros_like(bhv['state'])\n",
    "    one_hot_reg_factors.loc[bhv['state'] == 3, 'state_c'] = 1\n",
    "\n",
    "    one_hot_reg_factors['value'] = bhv['ch_val'].copy()\n",
    "\n",
    "    one_hot_reg_factors['state_a_val'] = one_hot_reg_factors['state_a'].values * bhv['ch_val'].values\n",
    "    one_hot_reg_factors['state_b_val'] = one_hot_reg_factors['state_b'].values * bhv['ch_val'].values\n",
    "    one_hot_reg_factors['state_c_val'] = one_hot_reg_factors['state_c'].values * bhv['ch_val'].values\n",
    "\n",
    "    # Define the factors (excluding intercept)\n",
    "    ix = (bhv['n_sacc'] == 1)\n",
    "    factors = ['state_a', 'state_b', 'state_c', 'value', 'state_a_val', 'state_b_val', 'state_c_val']\n",
    "\n",
    "    n_factors = len(factors)\n",
    "\n",
    "    # Initialize arrays to store results of time-resolved regression\n",
    "    t_factor_pvals = np.full((n_units, n_times, n_factors), np.nan)\n",
    "    t_factor_betas = np.full((n_units, n_times, n_factors), np.nan)\n",
    "\n",
    "\n",
    "    # initialize an array to accumulate p values from the mean periods into\n",
    "    choice_pvals = np.zeros((n_units, 3))\n",
    "    cue_pvals = np.zeros((n_units, 3))\n",
    "\n",
    "    reg_betas = np.zeros((n_units, 3))\n",
    "    reg_pvals = np.zeros((n_units, 3))\n",
    "\n",
    "\n",
    "    # initialize array for accumulating condition mean firing rates into\n",
    "    f_cond_means = np.zeros((12, n_units))\n",
    "\n",
    "    # initialize a dataframe for running an anova\n",
    "    anova_df = pd.DataFrame()\n",
    "    anova_df['state'] = bhv['state'].loc[ix]\n",
    "    anova_df['val'] = bhv['ch_val'].loc[ix]\n",
    "\n",
    "    t_anova_df = pd.DataFrame()\n",
    "    t_anova_df['state'] = bhv['state']\n",
    "    t_anova_df['cue'] = bhv['state_cue']\n",
    "    t_anova_df['val'] = bhv['ch_val']\n",
    "\n",
    "    # loop over the neurons\n",
    "    for u in tqdm(range(n_units)):\n",
    "        \n",
    "        # add the firing rates to the anova\n",
    "        anova_df['choice_fr'] = choice_frs[ix, u]\n",
    "        anova_df['cue_fr'] = cue_frs[ix, u]\n",
    "        \n",
    "        # run the choice anova\n",
    "        choice_anova_mdl = pg.anova(dv='choice_fr', between=['state', 'val'], data=anova_df)\n",
    "        choice_pvals[u,:] = choice_anova_mdl['p-unc'].values[0:3]\n",
    "        \n",
    "        # run the cue anova\n",
    "        cue_anova_mdl = pg.anova(dv='cue_fr', between=['state', 'val'], data=anova_df)\n",
    "        cue_pvals[u,:] = cue_anova_mdl['p-unc'].values[0:3]\n",
    "            \n",
    "        # run value-in-state regressions\n",
    "        state1_reg = pg.linear_regression(anova_df['val'].loc[ix & s1_ix], anova_df['choice_fr'].loc[ix & s1_ix])\n",
    "        reg_pvals[u, 0] = state1_reg['pval'].values[1]\n",
    "        reg_betas[u, 0] = state1_reg['coef'].values[1]\n",
    "        \n",
    "        state2_reg = pg.linear_regression(anova_df['val'].loc[ix & s2_ix], anova_df['choice_fr'].loc[ix & s2_ix])\n",
    "        reg_pvals[u, 1] = state2_reg['pval'].values[1]\n",
    "        reg_betas[u, 1] = state2_reg['coef'].values[1]\n",
    "        \n",
    "        state3_reg = pg.linear_regression(anova_df['val'].loc[ix & s3_ix], anova_df['choice_fr'].loc[ix & s3_ix])\n",
    "        reg_pvals[u, 2] = state3_reg['pval'].values[1]\n",
    "        reg_betas[u, 2] = state3_reg['coef'].values[1]\n",
    "\n",
    "        for t in range(n_times):\n",
    "            \n",
    "            # Grab this neuron's firing rate at this time\n",
    "            one_hot_reg_factors['firing_rate'] = firing_rates[:, t, u]\n",
    "            \n",
    "            # Run the regression\n",
    "            model = smf.ols('firing_rate ~ state_a + state_b + state_c + value + state_a_val + state_b_val + state_c_val', \n",
    "                        data=one_hot_reg_factors.loc[ix]).fit()\n",
    "            \n",
    "            # Extract p-values and betas for each factor\n",
    "            for i, factor in enumerate(factors):\n",
    "                if factor in model.params.index:\n",
    "                    t_factor_betas[u, t, i] = model.params[factor]\n",
    "                    t_factor_pvals[u, t, i] = model.pvalues[factor]\n",
    "        \n",
    "            \n",
    "    # let's try to understand the state code better with an AUROC analysis\n",
    "\n",
    "    # initialize arrays to accumulate results into\n",
    "    auc_scores = np.zeros((n_units, n_times, 4))\n",
    "    shuffle_auc_scores = np.zeros((n_units, n_times, 4)) \n",
    "\n",
    "    # let's only look at the single-saccade trials for this\n",
    "    b_triasl2balance = bhv['n_sacc']==1\n",
    "\n",
    "    # pull a balanced set of trials\n",
    "    trials2use, leftover_ix = pull_balanced_train_set(b_triasl2balance, [bhv['state'].values, bhv['state_cue'].values])\n",
    "\n",
    "    # grab the firing rates and behavioral data associated with these trials\n",
    "    b_fr = firing_rates[trials2use, :, :]\n",
    "    b_state_label = bhv['state'].loc[trials2use]\n",
    "\n",
    "    # now loop over each neuron\n",
    "    for u in tqdm(range(n_units)):\n",
    "        \n",
    "        # loop over times\n",
    "        for t in range(n_times):\n",
    "            \n",
    "            # run one-vs-all AUC classifiers for each state\n",
    "            auc_scores[u, t, 0] = roc_auc_score(b_state_label == 1, b_fr[:,t,u])\n",
    "            auc_scores[u, t, 1] = roc_auc_score(b_state_label == 2, b_fr[:,t,u])\n",
    "            auc_scores[u, t, 2] = roc_auc_score(b_state_label == 3, b_fr[:,t,u])\n",
    "            \n",
    "            # also look just at the state 1 and 2 trials\n",
    "            auc_scores[u, t, 3] = roc_auc_score(b_state_label[b_state_label < 3] == 1, b_fr[b_state_label < 3,t,u])\n",
    "            \n",
    "            # run the shuffles\n",
    "            # shuffle the labels\n",
    "            shuff_b_labels = np.random.permutation(b_state_label)\n",
    "            \n",
    "            # run one-vs-all AUC classifiers for each state\n",
    "            shuffle_auc_scores[u, t, 0] = roc_auc_score(shuff_b_labels == 1, b_fr[:,t,u])\n",
    "            shuffle_auc_scores[u, t, 1] = roc_auc_score(shuff_b_labels == 2, b_fr[:,t,u])\n",
    "            shuffle_auc_scores[u, t, 2] = roc_auc_score(shuff_b_labels == 3, b_fr[:,t,u])\n",
    "            \n",
    "            # also look just at the state 1 and 2 trials\n",
    "            shuffle_auc_scores[u, t, 3] = roc_auc_score(shuff_b_labels[shuff_b_labels < 3] == 1, b_fr[shuff_b_labels < 3,t,u])            \n",
    "                              \n",
    "    # rectify the scores\n",
    "    auc_scores[auc_scores < .5] = 1 - auc_scores[auc_scores < .5]\n",
    "    shuffle_auc_scores[shuffle_auc_scores < .5] = 1 - shuffle_auc_scores[shuffle_auc_scores < .5]   \n",
    "            \n",
    "    # now save the file\n",
    "    print('Saving data...')\n",
    "    save_name = save_dir + file_names[f_ix][0:-3] + '_summary.h5'\n",
    "\n",
    "    # Open an HDF5 file in write mode ('w' or 'w-' to create or truncate the file)\n",
    "    with h5py.File(save_name, 'w') as file:\n",
    "        # Create datasets within the HDF5 file and write data\n",
    "        file.create_dataset('brain_area', data=brain_areas)  \n",
    "        file.create_dataset('ts', data=ts)  \n",
    "        file.create_dataset('t_factor_pvals', data=t_factor_pvals) \n",
    "        file.create_dataset('t_factor_betas', data=t_factor_betas)   \n",
    "        file.create_dataset('valstate_pvals', data=reg_pvals)  \n",
    "        file.create_dataset('valstate_betas', data=reg_betas)  \n",
    "        file.create_dataset('state_auc_scores', data=auc_scores)\n",
    "        file.create_dataset('shuffle_auc_scores', data=shuffle_auc_scores)\n",
    "        \n",
    "    print('Data saved \\n')\n",
    "\n",
    "print('All files processed :]')\n",
    "\n",
    "    \n",
    "            \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
