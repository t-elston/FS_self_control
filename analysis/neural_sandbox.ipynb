{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# neural analysis\n",
    "Assesses data from neuropixels recordings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports \n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import statsmodels.api as sm\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.decomposition import PCA\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"Mean of empty slice\", category=RuntimeWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions\n",
    "def get_labelled_posteriors(indata, labels):\n",
    "\n",
    "    '''\n",
    "    INPUTS:\n",
    "    indata = posterior probabilites from a classifier with the shape\n",
    "            n_trials x n_timesteps x n_classes\n",
    "        \n",
    "    labels = 1d array with len(n_trials) - these labels ought\n",
    "            to correspond to class numbers (layers in indata)\n",
    "\n",
    "    OUTPUT:\n",
    "        labelled_posteriors = posterior probabilities associated with the\n",
    "        classes in the labels input for each timestep and trial\n",
    "    '''\n",
    "\n",
    "    n_trials, n_times, n_classes = indata.shape\n",
    "    class_lbls = np.unique(labels)\n",
    "    class_lbls = class_lbls[~np.isnan(class_lbls)]\n",
    "\n",
    "    # initialize output\n",
    "    labelled_posteriors = np.zeros(shape = (n_trials, n_times))\n",
    "\n",
    "    for ix, lbl in enumerate(class_lbls):\n",
    "        \n",
    "        # find trials where this label was chosen\n",
    "        labelled_posteriors[labels == lbl,:] = indata[labels == lbl,:,int(ix)]\n",
    "        \n",
    "    return labelled_posteriors\n",
    "\n",
    "\n",
    "def pull_balanced_train_set(trials2balance, params2balance):\n",
    "    '''\n",
    "    INPUTS:\n",
    "    trials2balance   - ***logical array*** of the trials you want to balance\n",
    "    params2balance   - ***list*** where each element is a vector of categorical\n",
    "                        parameters to balance (e.g. choice value and side)\n",
    "                        each element of params2balance must have the same\n",
    "                        number of elements as trials2balance\n",
    "    OUTPUTS:\n",
    "    train_ix         - trial indices of a fully balanced training set\n",
    "    leftover_ix      - trial indices of trials not included in train_ix\n",
    "    '''\n",
    "\n",
    "    # Find the indices where trials are selected to balance\n",
    "    balance_indices = np.where(trials2balance)[0]\n",
    "\n",
    "    # Create an array of parameters to balance\n",
    "    params_array = np.array(params2balance).T\n",
    "\n",
    "    # Find unique combinations and their counts\n",
    "    p_combos, p_counts = np.unique(params_array[balance_indices], axis=0, return_counts=True)\n",
    "\n",
    "    # Determine the minimum count for a balanced set\n",
    "    n_to_keep = np.min(p_counts)\n",
    "\n",
    "    # Initialize arrays to mark selected and leftover trials\n",
    "    train_ix = np.zeros(len(trials2balance), dtype=bool)\n",
    "    leftover_ix = np.zeros(len(trials2balance), dtype=bool)\n",
    "\n",
    "    # Select a balanced number of trials for each unique parameter combination\n",
    "    for combo in p_combos:\n",
    "        # Find indices of trials corresponding to the current combination\n",
    "        combo_indices = np.where((params_array == combo).all(axis=1) & trials2balance)[0]\n",
    "\n",
    "        # Shuffle the indices\n",
    "        np.random.shuffle(combo_indices)\n",
    "\n",
    "        # Select n_to_keep trials and mark them as part of the training set\n",
    "        train_ix[combo_indices[:n_to_keep]] = True\n",
    "\n",
    "        # Mark the remaining trials as leftovers\n",
    "        leftover_ix[combo_indices[n_to_keep:]] = True\n",
    "\n",
    "    return train_ix, leftover_ix\n",
    "\n",
    "\n",
    "def random_prop_of_array(inarray, proportion):\n",
    "    '''\n",
    "    INPUTS\n",
    "    inarray = logical/boolean array of indices to potentially use later\n",
    "    proportion = how much of inarray should randomly be selected\n",
    "\n",
    "    OUTPUT\n",
    "    out_array = logical/boolean that's set as 'true' for a proportion of the \n",
    "                initial 'true' values in inarray\n",
    "    '''\n",
    "\n",
    "    out_array = np.zeros(shape = (len(inarray), ))\n",
    "\n",
    "    # find where inarray is true and shuffle those indices\n",
    "    shuffled_ixs = np.random.permutation(np.asarray(np.where(inarray)).flatten())\n",
    "\n",
    "    # keep only a proportion of that array\n",
    "    kept_ix = shuffled_ixs[0: round(len(shuffled_ixs)*proportion)]\n",
    "\n",
    "    # fill in the kept indices\n",
    "    out_array[kept_ix] = 1\n",
    "\n",
    "    # make this a logical/boolean\n",
    "    out_array = out_array > 0\n",
    "\n",
    "    return out_array\n",
    "\n",
    "\n",
    "def pull_from_h5(file_path, data_to_extract):\n",
    "    try:\n",
    "        with h5py.File(file_path, 'r') as file:\n",
    "            # Check if the data_to_extract exists in the HDF5 file\n",
    "            if data_to_extract in file:\n",
    "                data = file[data_to_extract][...]  # Extract the data\n",
    "                return data\n",
    "            else:\n",
    "                print(f\"'{data_to_extract}' not found in the file.\")\n",
    "                return None\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return None\n",
    "    \n",
    "def list_hdf5_data(file_path):\n",
    "    try:\n",
    "        with h5py.File(file_path, 'r') as file:\n",
    "            print(f\"Datasets in '{file_path}':\")\n",
    "            for dataset in file:\n",
    "                print(dataset)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "\n",
    "def get_ch_and_unch_vals(bhv):\n",
    "    \"\"\"\n",
    "    Extracts chosen (ch_val) and unchosen (unch_val) values associated with each trial.\n",
    "\n",
    "    Parameters:\n",
    "    - bhv (DataFrame): DataFrame behavioral data.\n",
    "\n",
    "    Returns:\n",
    "    - ch_val (ndarray): Array of chosen values for each trial.\n",
    "    - unch_val (ndarray): Array of unchosen values for each trial. \n",
    "                          - places 0s for unchosen values on forced choice trials\n",
    "    \"\"\"\n",
    "    ch_val = np.zeros(shape=(len(bhv, )))\n",
    "    unch_val = np.zeros(shape=(len(bhv, )))\n",
    "\n",
    "    bhv['r_val'] = bhv['r_val'].fillna(0)\n",
    "    bhv['l_val'] = bhv['l_val'].fillna(0)\n",
    "\n",
    "    ch_left = bhv['side'] == -1\n",
    "    ch_right = bhv['side'] == 1\n",
    "\n",
    "    ch_val[ch_left] = bhv['l_val'].loc[ch_left].astype(int)\n",
    "    ch_val[ch_right] = bhv['r_val'].loc[ch_right].astype(int)\n",
    "\n",
    "    unch_val[ch_left] = bhv['r_val'].loc[ch_left].astype(int)\n",
    "    unch_val[ch_right] = bhv['l_val'].loc[ch_right].astype(int)\n",
    "\n",
    "    return ch_val, unch_val\n",
    "\n",
    "\n",
    "def get_ch_and_unch_pps(in_pp, bhv, ch_val, unch_val):\n",
    "    \"\"\"Gets the posteriors associated with the chosen and unchosen classes\n",
    "\n",
    "    Args:\n",
    "        in_pp (ndarray): array of posteriors (n_trials x n_times x n_classes)\n",
    "        bhv (dataframe): details of each trial\n",
    "        ch_val (ndarray): vector indicating the class that is ultimately chosen\n",
    "        unch_val (ndarray): vector indicating the class that was ultimately not chosen\n",
    "\n",
    "    Returns:\n",
    "        ch_pp (ndarray): vector of the postior at each point in time for each trial's chosen option\n",
    "        unch_pp (ndarray): vector of the postior at each point in time for each trial's unchosen option\n",
    "    \"\"\"\n",
    "\n",
    "    # select the chosen and unchosen values \n",
    "    n_trials, n_times, n_classes = np.shape(in_pp)\n",
    "    ch_pp = np.zeros(shape=(n_trials, n_times))\n",
    "    unch_pp = np.zeros(shape=(n_trials, n_times))\n",
    "\n",
    "    # loop over each trial\n",
    "    for t in range(n_trials):\n",
    "        \n",
    "        # get the chosen and unchosen PPs\n",
    "        ch_pp[t, :] = in_pp[t, :, int(ch_val[t]-1)]\n",
    "        unch_pp[t, :] = in_pp[t, :, int(unch_val[t]-1)]\n",
    "        \n",
    "    # set the forced choice unchosen pps to nans, since there was only 1 option\n",
    "    unch_pp[bhv['forced'] == 1, :] = np.nan\n",
    "    \n",
    "    return ch_pp, unch_pp\n",
    "\n",
    "\n",
    "def get_alt_ch_and_unch_pps(in_pp, bhv, s_ch_val, s_unch_val):\n",
    "    \"\"\"Gets the posteriors associated with the chosen and unchosen classes\n",
    "\n",
    "    Args:\n",
    "        in_pp (ndarray): array of posteriors (n_trials x n_times x n_classes)\n",
    "        bhv (dataframe): details of each trial\n",
    "        s_ch_val (ndarray): vector indicating the class that is ultimately chosen\n",
    "        s_unch_val (ndarray): vector indicating the class that was ultimately not chosen\n",
    "\n",
    "    Returns:\n",
    "        alt_ch_pp (ndarray): vector of the postior at each point in time for the alternative value in the other state\n",
    "        alt_unch_pp (ndarray): vector of the postior at each point in time for the alternative value in the other state\n",
    "    \"\"\"\n",
    "\n",
    "    # select the chosen and unchosen values \n",
    "    n_trials, n_times, n_classes = np.shape(in_pp)\n",
    "    alt_ch_pp = np.zeros(shape=(n_trials, n_times))\n",
    "    alt_unch_pp = np.zeros(shape=(n_trials, n_times))\n",
    "\n",
    "    alt_ch_val = np.zeros_like(s_ch_val)\n",
    "    alt_unch_val = np.zeros_like(s_unch_val)\n",
    "    \n",
    "    alt_ch_val[bhv['state'] == 1] = 8 - s_ch_val[bhv['state'] == 1] + 1\n",
    "    alt_ch_val[bhv['state'] == 2] = 8 - s_ch_val[bhv['state'] == 2] + 1\n",
    "\n",
    "    alt_unch_val[bhv['state'] == 1] = 8 - s_unch_val[bhv['state'] == 1] + 1\n",
    "    alt_unch_val[bhv['state'] == 2] = 8 - s_unch_val[bhv['state'] == 2] + 1\n",
    "\n",
    "    for t in range(n_trials):\n",
    "        \n",
    "        alt_ch_pp[t, :] = in_pp[t, :, int(alt_ch_val[t]-1)]\n",
    "        alt_unch_pp[t, :] = in_pp[t, :, int(alt_unch_val[t]-1)]\n",
    "\n",
    "    # set the alternative values to nans for state 3, since there were no alternatives\n",
    "    alt_ch_pp[bhv['state'] == 3] = np.nan\n",
    "    alt_unch_pp[bhv['state'] == 3] = np.nan\n",
    "\n",
    "    return alt_ch_pp, alt_unch_pp\n",
    "\n",
    "def find_candidate_states(indata, n_classes, temporal_thresh, mag_thresh):\n",
    "    \"\"\"Finds periods where decoded posteriors are twice their noise level.\n",
    "\n",
    "    Args:\n",
    "        indata (ndarray): 2d array of posterior probabilities associated with some decoder output.\n",
    "        n_classes (int): How many classes were used in the decoder?\n",
    "        temporal_thresh (int): Number of contiguous samples that must be above a threshold to be a real state (typically 2).\n",
    "        mag_thresh (flat): how many times the noise level must a state be? (e.g. 2 = twice the noise level)\n",
    "\n",
    "    Returns:\n",
    "        state_details (ndarray): 2d array where each row details when a state occurred [trial_num, time_in_trial, state_length].\n",
    "        state_array (ndarray): 2d array the same size as indata. It contains 1 in all locations where there were states and 0s everywhere else.\n",
    "    \"\"\"\n",
    "    state_details = np.array([])\n",
    "    state_array = np.zeros_like(indata)\n",
    "    \n",
    "    state_magnitude_thresh = (1 / n_classes) * mag_thresh\n",
    "\n",
    "    for t in range(indata.shape[0]):\n",
    "        state_len, state_pos, state_type = find_1dsequences(indata[t, :] > state_magnitude_thresh)\n",
    "        state_len = state_len[state_type == True]\n",
    "        state_pos = state_pos[state_type == True]\n",
    "\n",
    "        for i in range(len(state_len)):\n",
    "            state_details = np.concatenate((state_details, np.array([t, state_pos[i], state_len[i]])))\n",
    "\n",
    "    state_details = state_details.reshape(-1, 3)\n",
    "    state_details = state_details[state_details[:, 2] > temporal_thresh, :]\n",
    "\n",
    "    # Update state_array using state_details information\n",
    "    for j in range(len(state_details)):\n",
    "        state_trial, state_start, state_len = state_details[j].astype(int)\n",
    "        state_array[state_trial, state_start:(state_start + state_len)] = 1\n",
    "\n",
    "    return state_details, state_array\n",
    "\n",
    "def moving_average(x, w, axis=0):\n",
    "    '''\n",
    "    Moving average function that operates along specified dimensions of a NumPy array.\n",
    "\n",
    "    Parameters:\n",
    "    - x (numpy.ndarray): Input array.\n",
    "    - w (int): Size of the window to convolve the array with (i.e., smoothness factor).\n",
    "    - axis (int): Axis along which to perform the moving average (default is 0).\n",
    "\n",
    "    Returns:\n",
    "    - numpy.ndarray: Smoothed array along the specified axis with the same size as the input array.\n",
    "    '''\n",
    "    x = np.asarray(x)  # Ensure input is a NumPy array\n",
    "    if np.isnan(x).any():\n",
    "        x = np.nan_to_num(x)  # Replace NaN values with zeros\n",
    "\n",
    "    if axis < 0:\n",
    "        axis += x.ndim  # Adjust negative axis value\n",
    "\n",
    "    kernel = np.ones(w) / w  # Create kernel for moving average\n",
    "\n",
    "    # Pad the array before applying convolution\n",
    "    pad_width = [(0, 0)] * x.ndim  # Initialize padding for each axis\n",
    "    pad_width[axis] = (w - 1, 0)  # Pad along the specified axis (left side)\n",
    "    x_padded = np.pad(x, pad_width, mode='constant', constant_values=0)\n",
    "\n",
    "    # Apply 1D convolution along the specified axis on the padded array\n",
    "    return np.apply_along_axis(lambda m: np.convolve(m, kernel, mode='valid'), axis, x_padded)\n",
    "\n",
    "def find_1dsequences(inarray):\n",
    "        ''' \n",
    "        run length encoding. Partial credit to R rle function. \n",
    "        Multi datatype arrays catered for including non Numpy\n",
    "        returns: tuple (runlengths, startpositions, values) \n",
    "        '''\n",
    "        ia = np.asarray(inarray)                # force numpy\n",
    "        n = len(ia)\n",
    "        if n == 0: \n",
    "            return (None, None, None)\n",
    "        else:\n",
    "            y = ia[1:] != ia[:-1]                 # pairwise unequal (string safe)\n",
    "            i = np.append(np.where(y), n - 1)     # must include last element \n",
    "            lens = np.diff(np.append(-1, i))      # run lengths\n",
    "            pos = np.cumsum(np.append(0, lens))[:-1] # positions\n",
    "            return(lens, pos, ia[i])\n",
    "  \n",
    "        \n",
    "def shuffle_along_axis(arr, axis):\n",
    "    return np.apply_along_axis(np.random.permutation, axis, arr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'C:/Users/thome/Documents/PYTHON/Self-Control/raw_data/K20240707_Rec06.h5'\n",
    "save_dir = 'C:/Users/thome/Documents/PYTHON/Self-Control/lr_decoder_output/' \n",
    "save_data = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# access the data for this session\n",
    "firing_rates = np.concatenate([pull_from_h5(file_path, 'CdN_zFR'), \n",
    "                               pull_from_h5(file_path, 'OFC_zFR')], axis=2)\n",
    "\n",
    "u_names = np.concatenate([pull_from_h5(file_path, 'CdN_u_names'), \n",
    "                          pull_from_h5(file_path, 'OFC_u_names')], axis=0)\n",
    "\n",
    "n_OFC = pull_from_h5(file_path, 'OFC_zFR').shape[2]\n",
    "n_CdN = pull_from_h5(file_path, 'CdN_zFR').shape[2]\n",
    "brain_areas = np.concatenate([np.zeros(shape=n_CdN, ), np.ones(shape=n_OFC, )]).astype(int)\n",
    "\n",
    "ts = pull_from_h5(file_path, 'ts')\n",
    "bhv = pd.read_hdf(file_path, key='bhv')\n",
    "\n",
    "if len(bhv) > len(firing_rates):\n",
    "    bhv = bhv.loc[0 :len(firing_rates)-1]\n",
    "\n",
    "firing_rates = np.nan_to_num(firing_rates, nan=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull the relevant labels\n",
    "ch_val, unch_val = get_ch_and_unch_vals(bhv)\n",
    "\n",
    "# set up a state-independent value decoder\n",
    "n_boots = 1000\n",
    "n_trials = len(bhv)\n",
    "n_vals = len(np.unique(ch_val))\n",
    "\n",
    "# run a state x value classifier\n",
    "\n",
    "# get the labels associated with each unique state-value pair\n",
    "ch_val, unch_val = get_ch_and_unch_vals(bhv)\n",
    "s_ch_val = ch_val.copy()\n",
    "s_unch_val = unch_val.copy()\n",
    "\n",
    "s_ch_val[bhv['state'] == 2] = s_ch_val[bhv['state'] == 2] + 4\n",
    "s_ch_val[bhv['state'] == 3] = s_ch_val[bhv['state'] == 3] + 8\n",
    "s_unch_val[bhv['state'] == 2] = s_unch_val[bhv['state'] == 2] + 4\n",
    "s_unch_val[bhv['state'] == 3] = s_unch_val[bhv['state'] == 3] + 8\n",
    "\n",
    "n_vals = len(np.unique(s_ch_val))\n",
    "\n",
    "# initialize arrays to accumulate results into\n",
    "OFC_pp = np.zeros(shape=(n_trials, len(ts), n_vals, n_boots), dtype=np.float32)\n",
    "CdN_pp = np.zeros(shape=(n_trials, len(ts), n_vals, n_boots), dtype=np.float32)\n",
    "\n",
    "OFC_acc = np.zeros(shape=(n_trials, len(ts), n_boots), dtype=np.float32)\n",
    "CdN_acc = np.zeros(shape=(n_trials, len(ts), n_boots), dtype=np.float32)\n",
    "\n",
    "# set those arrays to nans\n",
    "OFC_pp[:] = np.nan\n",
    "CdN_pp[:] = np.nan\n",
    "\n",
    "OFC_acc[:] = np.nan\n",
    "CdN_acc[:] = np.nan\n",
    "\n",
    "ofc_ix = brain_areas == 1\n",
    "cdn_ix = brain_areas == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # loop over bootstraps\n",
    "# for b in tqdm(range(n_boots)):\n",
    "\n",
    "#     # partition the data into training and testing sets\n",
    "#     train_trials = (bhv['forced'] == 1) | (bhv['n_sacc'] == 1)\n",
    "#     # b_triasl2balance = random_prop_of_array(train_trials, .9)\n",
    "#     # train_ix, leftover_ix = pull_balanced_train_set(b_triasl2balance, [ch_val, bhv['state'].values])\n",
    "    \n",
    "#     train_ix = random_prop_of_array(train_trials, .9)\n",
    "\n",
    "#     train_labels = s_ch_val[train_ix]\n",
    "#     train_fr = firing_rates[train_ix, :, :]\n",
    "#     train_fr = np.nan_to_num(train_fr, nan=0)\n",
    "    \n",
    "#     # shuffle the training labels\n",
    "#     s_train_labels = np.random.permutation(train_labels)\n",
    "    \n",
    "#     if len(np.unique(train_labels)) == len(np.unique(s_ch_val)):\n",
    "\n",
    "#         # loop over time steps\n",
    "#         for t in range(len(ts)-1):\n",
    "            \n",
    "#             # initialize classifiers\n",
    "#             OFC_lda = LinearDiscriminantAnalysis()\n",
    "#             CdN_lda = LinearDiscriminantAnalysis()\n",
    "\n",
    "#             # train the classifiers\n",
    "#             OFC_lda.fit(train_fr[:, t,  brain_areas == 1], train_labels)\n",
    "#             CdN_lda.fit(train_fr[:, t,  brain_areas == 0], train_labels)\n",
    "\n",
    "#             # test the classifiers \n",
    "#             OFC_pp[:, t, :, b] = OFC_lda.predict_proba(firing_rates[:, t, ofc_ix])\n",
    "#             CdN_pp[:, t, :, b] = CdN_lda.predict_proba(firing_rates[:, t, cdn_ix])\n",
    "            \n",
    "#             OFC_acc[:, t, b] = OFC_lda.predict(firing_rates[:, t, ofc_ix]) == s_ch_val\n",
    "#             CdN_acc[:, t, b] = CdN_lda.predict(firing_rates[:, t, cdn_ix]) == s_ch_val\n",
    "            \n",
    "#             # # shuffle the firing rates and test again\n",
    "#             # s_OFC_pp[:, t, :, b] = OFC_lda.predict_proba(shuffle_along_axis(firing_rates[:, t, ofc_ix], axis=0))\n",
    "#             # s_CdN_pp[:, t, :, b] = CdN_lda.predict_proba(shuffle_along_axis(firing_rates[:, t, cdn_ix], axis=0))\n",
    "\n",
    "#         # set the training trials to nans \n",
    "#         OFC_pp[train_ix, :,:, b] = np.nan\n",
    "#         CdN_pp[train_ix, :,:, b] = np.nan\n",
    "        \n",
    "#         # s_OFC_pp[train_ix, :,:, b] = np.nan\n",
    "#         # s_CdN_pp[train_ix, :,:, b] = np.nan\n",
    "        \n",
    "#         OFC_acc[train_ix, :,b] = np.nan\n",
    "#         CdN_acc[train_ix, :,b] = np.nan\n",
    "\n",
    "# # average over the bootstraps for the real data\n",
    "# OFC_pp_mean = np.nanmean(OFC_pp, axis=3)\n",
    "# CdN_pp_mean = np.nanmean(CdN_pp, axis=3)\n",
    "\n",
    "# OFC_acc_mean = np.nanmean(OFC_acc, axis=2)\n",
    "# CdN_acc_mean = np.nanmean(CdN_acc, axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_bootstrap(b):\n",
    "    \"\"\"\n",
    "    Perform a single bootstrap iteration, including training and evaluating classifiers.\n",
    "    \"\"\"\n",
    "    # Partition the data into training and testing sets\n",
    "    train_trials = (bhv['forced'] == 1) | (bhv['n_sacc'] == 1)\n",
    "    train_ix = random_prop_of_array(train_trials, .8)\n",
    "\n",
    "    train_labels = s_ch_val[train_ix]\n",
    "    train_fr = firing_rates[train_ix, :, :]\n",
    "    train_fr = np.nan_to_num(train_fr, nan=0)\n",
    "\n",
    "    # Shuffle the training labels for null distribution\n",
    "    #s_train_labels = np.random.permutation(train_labels)\n",
    "\n",
    "    # Check if all values are present\n",
    "    if len(np.unique(train_labels)) == len(np.unique(s_ch_val)):\n",
    "        \n",
    "        # Initialize arrays to accumulate results\n",
    "        OFC_pp_temp = np.full((n_trials, len(ts), n_vals), np.nan, dtype=np.float32)\n",
    "        CdN_pp_temp = np.full((n_trials, len(ts), n_vals), np.nan, dtype=np.float32)\n",
    "        \n",
    "        OFC_acc_temp = np.full((n_trials, len(ts)), np.nan, dtype=np.float32)\n",
    "        CdN_acc_temp = np.full((n_trials, len(ts)), np.nan, dtype=np.float32)\n",
    "        \n",
    "        # Loop over time steps\n",
    "        for t in range(len(ts)-1):\n",
    "            # # Initialize classifiers\n",
    "            # OFC_lda = LinearDiscriminantAnalysis(priors=np.ones(n_vals) / n_vals)\n",
    "            # CdN_lda = LinearDiscriminantAnalysis(priors=np.ones(n_vals) / n_vals)\n",
    "            \n",
    "            OFC_lda = LogisticRegression(penalty='l2', solver='lbfgs', max_iter=1000)\n",
    "            CdN_lda = LogisticRegression(penalty='l2', solver='lbfgs', max_iter=1000)\n",
    "\n",
    "            # Train the classifiers\n",
    "            OFC_lda.fit(train_fr[:, t, brain_areas == 1], train_labels)\n",
    "            CdN_lda.fit(train_fr[:, t, brain_areas == 0], train_labels)\n",
    "\n",
    "            # Test the classifiers\n",
    "            OFC_pp_temp[:, t, :] = OFC_lda.predict_proba(firing_rates[:, t, ofc_ix])\n",
    "            CdN_pp_temp[:, t, :] = CdN_lda.predict_proba(firing_rates[:, t, cdn_ix])\n",
    "\n",
    "            OFC_acc_temp[:, t] = OFC_lda.predict(firing_rates[:, t, ofc_ix]) == s_ch_val\n",
    "            CdN_acc_temp[:, t] = CdN_lda.predict(firing_rates[:, t, cdn_ix]) == s_ch_val\n",
    "\n",
    "        # Set training trials to NaNs\n",
    "        OFC_pp_temp[train_ix, :, :] = np.nan\n",
    "        CdN_pp_temp[train_ix, :, :] = np.nan\n",
    "\n",
    "        OFC_acc_temp[train_ix, :] = np.nan\n",
    "        CdN_acc_temp[train_ix, :] = np.nan\n",
    "\n",
    "        return OFC_pp_temp, CdN_pp_temp, OFC_acc_temp, CdN_acc_temp\n",
    "    else:\n",
    "        # Return None if condition not met\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|â–Ž         | 18/500 [00:14<05:45,  1.39it/s]"
     ]
    }
   ],
   "source": [
    "# Run bootstraps in parallel\n",
    "results = Parallel(n_jobs=6)(delayed(perform_bootstrap)(b) for b in tqdm(range(n_boots)))\n",
    "\n",
    "# Aggregate results\n",
    "for b, res in enumerate(results):\n",
    "    if res is not None:\n",
    "        OFC_pp[:, :, :, b], CdN_pp[:, :, :, b], OFC_acc[:, :, b], CdN_acc[:, :, b] = res\n",
    "        \n",
    "# Compute mean over bootstraps \n",
    "OFC_pp_mean = np.nanmean(OFC_pp, axis=3)\n",
    "CdN_pp_mean = np.nanmean(CdN_pp, axis=3)\n",
    "\n",
    "OFC_acc_mean = np.nanmean(OFC_acc, axis=2)\n",
    "CdN_acc_mean = np.nanmean(CdN_acc, axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the chosen and unchosen posteriors\n",
    "OFC_ch, OFC_unch = get_ch_and_unch_pps(OFC_pp_mean, bhv, s_ch_val, s_unch_val)\n",
    "CdN_ch, CdN_unch = get_ch_and_unch_pps(CdN_pp_mean, bhv, s_ch_val, s_unch_val)\n",
    "\n",
    "# get values associated with competing state\n",
    "OFC_alt_ch, OFC_alt_unch = get_alt_ch_and_unch_pps(OFC_pp_mean, bhv, s_ch_val, s_unch_val)\n",
    "CdN_alt_ch, CdN_alt_unch = get_alt_ch_and_unch_pps(CdN_pp_mean, bhv, s_ch_val, s_unch_val)\n",
    "\n",
    "# get the alternative values associated with each option\n",
    "ch_val, unch_val = get_ch_and_unch_vals(bhv)\n",
    "ix = (bhv['n_sacc'] == 2) & (bhv['state'] == 2)\n",
    "\n",
    "# Create a figure and two subplots\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "ax1.plot(ts, np.nanmean(OFC_acc_mean[ix, :], axis=0), color='tab:red', label='OFC')\n",
    "ax1.plot(ts, np.nanmean(CdN_acc_mean[ix, :], axis=0), color='tab:blue', label='CdN')\n",
    "ax1.set_xlabel('Time from Pics On (ms)')\n",
    "ax1.set_ylabel('Accuracy')\n",
    "ax1.legend()\n",
    "\n",
    "ax2.plot(ts, np.nanmean(OFC_ch[ix, :], axis=0), color = 'tab:red', label='OFC_ch')\n",
    "ax2.plot(ts, np.nanmean(OFC_alt_ch[ix, :], axis=0), color = 'tab:red', label='OFC_alt_ch', linestyle='--')\n",
    "ax2.plot(ts, np.nanmean(OFC_unch[ix, :], axis=0), color = 'tab:red', linestyle=':', label='OFC_unch')\n",
    "ax2.plot(ts, np.nanmean(OFC_alt_unch[ix, :], axis=0), color = 'tab:red', marker='o', label='OFC_alt_unch')\n",
    "ax2.plot(ts, np.nanmean(CdN_ch[ix, :], axis=0), color = 'tab:blue', label='CdN_ch')\n",
    "ax2.plot(ts, np.nanmean(CdN_alt_ch[ix, :], axis=0), color = 'tab:blue', label='CdN_alt_ch', linestyle='--')\n",
    "ax2.plot(ts, np.nanmean(CdN_unch[ix, :], axis=0), color = 'tab:blue', linestyle=':', label='CdN_unch')\n",
    "ax2.plot(ts, np.nanmean(CdN_alt_unch[ix, :], axis=0), color = 'tab:blue', marker='o', label='CdN_alt_unch')\n",
    "ax2.set_xlabel('Time from Pics On (ms)')\n",
    "ax2.set_ylabel('Posterior Probability')\n",
    "ax2.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the data\n",
    "save_name = save_dir + Path(file_path).stem + '_decoder.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_data:\n",
    "    # Open an HDF5 file in write mode ('w' or 'w-' to create or truncate the file)\n",
    "    with h5py.File(save_name, 'w') as file:\n",
    "        # Create datasets within the HDF5 file and write data\n",
    "        file.create_dataset('OFC_acc_mean', data=OFC_acc_mean)  \n",
    "        file.create_dataset('OFC_ch', data=OFC_ch)  \n",
    "        file.create_dataset('OFC_unch', data=OFC_unch)  \n",
    "        file.create_dataset('OFC_alt_ch', data=OFC_alt_ch)  \n",
    "        file.create_dataset('OFC_alt_unch', data=OFC_alt_unch)  \n",
    "        file.create_dataset('OFC_pp', data=OFC_pp_mean)  \n",
    "        \n",
    "        file.create_dataset('CdN_acc_mean', data=CdN_acc_mean)  \n",
    "        file.create_dataset('CdN_ch', data=CdN_ch)  \n",
    "        file.create_dataset('CdN_unch', data=CdN_unch)  \n",
    "        file.create_dataset('CdN_alt_ch', data=CdN_alt_ch)  \n",
    "        file.create_dataset('CdN_alt_unch', data=CdN_alt_unch)  \n",
    "        file.create_dataset('CdN_pp', data=CdN_pp_mean)\n",
    "\n",
    "        file.create_dataset('ts', data= ts)\n",
    "        \n",
    "        # Save behavior to the HDF5 file\n",
    "        bhv.to_hdf(save_name, key='bhv', mode='a')\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
